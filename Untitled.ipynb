{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc23f032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories [nan] in column 0 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m grid_point_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mCategorical(grid_point_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m], categories\u001b[38;5;241m=\u001b[39mencoder\u001b[38;5;241m.\u001b[39mcategories_[\u001b[38;5;241m1\u001b[39m], ordered\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Feature engineering for grid points: One-hot encode categorical variables\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m encoded_features_grid \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_point_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLocation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMonth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m grid_point_encoded_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([grid_point_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]], pd\u001b[38;5;241m.\u001b[39mDataFrame(encoded_features_grid)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Split the dataset into features (X) and labels (y)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:877\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;66;03m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[0;32m    873\u001b[0m warn_on_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfrequent_if_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    876\u001b[0m }\n\u001b[1;32m--> 877\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_infrequent_categories(X_int, X_mask)\n\u001b[0;32m    885\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:174\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle_unknown \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    170\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound unknown categories \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m in column \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m during transform\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(diff, i)\n\u001b[0;32m    173\u001b[0m     )\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m warn_on_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: Found unknown categories [nan] in column 0 during transform"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from random import choice\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# Create a synthetic dataset\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate date range\n",
    "start_date = datetime(2022, 1, 1)\n",
    "end_date = datetime(2022, 12, 31)\n",
    "date_range = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n",
    "\n",
    "# Generate crime data\n",
    "n_samples = 1000\n",
    "crime_types = ['Theft', 'Assault', 'Vandalism', 'Burglary', 'Drug Offense']\n",
    "locations = ['Dormitory', 'Classroom Building', 'Library', 'Recreation Center', 'Cafeteria']\n",
    "months = [d.strftime('%B') for d in date_range]\n",
    "crime_data = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    date = choice(date_range)\n",
    "    crime_type = choice(crime_types)\n",
    "    location = choice(locations)\n",
    "    month = date.strftime('%B')\n",
    "    latitude = round(np.random.uniform(37.0, 37.1), 6)\n",
    "    longitude = round(np.random.uniform(-122.0, -121.9), 6)\n",
    "\n",
    "    crime_data.append([date, crime_type, location, month, latitude, longitude])\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = ['Date', 'Crime Type', 'Location', 'Month', 'Latitude', 'Longitude']\n",
    "df = pd.DataFrame(crime_data, columns=columns)\n",
    "\n",
    "# Feature engineering: One-hot encode categorical variables\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "encoded_features = encoder.fit_transform(df[['Location', 'Month']])\n",
    "encoded_df = pd.concat([df[['Latitude', 'Longitude']], pd.DataFrame(encoded_features)], axis=1)\n",
    "\n",
    "# Create a base map\n",
    "m = folium.Map(location=[df['Latitude'].mean(), df['Longitude'].mean()], zoom_start=10)\n",
    "\n",
    "# Add markers with intensity based on the number of unique crime incidents\n",
    "locations = list(zip(df['Latitude'], df['Longitude']))\n",
    "heat_map = HeatMap(locations, radius=15)\n",
    "heat_map.add_to(m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('crime_intensity_map.html')\n",
    "\n",
    "# Create a grid of latitude and longitude coordinates\n",
    "latitude_range = (df['Latitude'].min(), df['Latitude'].max())\n",
    "longitude_range = (df['Longitude'].min(), df['Longitude'].max())\n",
    "latitude_step = 0.01\n",
    "longitude_step = 0.01\n",
    "\n",
    "grid_points = []\n",
    "\n",
    "for lat in np.arange(latitude_range[0], latitude_range[1], latitude_step):\n",
    "    for lon in np.arange(longitude_range[0], longitude_range[1], longitude_step):\n",
    "        # Include 'Location' and 'Month' in grid_point_df\n",
    "        grid_points.append((lat, lon, choice(locations), choice(months)))\n",
    "\n",
    "# Create a DataFrame with grid points\n",
    "grid_point_df = pd.DataFrame(grid_points, columns=['Latitude', 'Longitude', 'Location', 'Month'])\n",
    "\n",
    "# Ensure every grid point has a valid month\n",
    "grid_point_df['Month'] = grid_point_df['Month'].apply(lambda x: choice(months) if pd.isna(x) else x)\n",
    "\n",
    "# Make sure the 'Location' and 'Month' columns in grid_point_df have the same categories as in the training data\n",
    "grid_point_df['Location'] = pd.Categorical(grid_point_df['Location'], categories=encoder.categories_[0], ordered=False)\n",
    "grid_point_df['Month'] = pd.Categorical(grid_point_df['Month'], categories=encoder.categories_[1], ordered=False)\n",
    "\n",
    "# Feature engineering for grid points: One-hot encode categorical variables\n",
    "encoded_features_grid = encoder.transform(grid_point_df[['Location', 'Month']])\n",
    "grid_point_encoded_df = pd.concat([grid_point_df[['Latitude', 'Longitude']], pd.DataFrame(encoded_features_grid)], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and labels (y)\n",
    "X = encoded_df.drop('Crime Type', axis=1)\n",
    "y = df['Crime Type']\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf.fit(X, y)\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(clf_rf, 'random_forest_model.pkl')\n",
    "\n",
    "# Load the trained model from the file\n",
    "clf_rf_loaded = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Use the loaded model to predict the crime types for all grid points\n",
    "predicted_crime_types = clf_rf_loaded.predict(grid_point_encoded_df)\n",
    "\n",
    "# Store the predicted incidents for each grid point\n",
    "predicted_incidents = {}\n",
    "\n",
    "for i, point in enumerate(grid_points):\n",
    "    predicted_incidents[point] = predicted_crime_types[i]\n",
    "\n",
    "# Create a heatmap for predicted crime incidents\n",
    "heat_map_predicted = HeatMap(\n",
    "    data=list(predicted_incidents.keys()),\n",
    "    radius=15,\n",
    "    min_opacity=0.2,\n",
    "    max_val=max(predicted_incidents.values()),\n",
    "    gradient={0.2: 'blue', 0.4: 'lime', 0.6: 'orange', 1: 'red'}\n",
    ")\n",
    "\n",
    "heat_map_predicted.add_to(m)\n",
    "\n",
    "# Save the map with predicted crime incidents to an HTML file\n",
    "m.save('predicted_crime_intensity_map.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b2379f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories [nan] in column 0 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m grid_point_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mCategorical(grid_point_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m], categories\u001b[38;5;241m=\u001b[39mencoder\u001b[38;5;241m.\u001b[39mcategories_[\u001b[38;5;241m1\u001b[39m], ordered\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Feature engineering for grid points: One-hot encode categorical variables\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m encoded_features_grid \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_point_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLocation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMonth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m grid_point_encoded_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([grid_point_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]], pd\u001b[38;5;241m.\u001b[39mDataFrame(encoded_features_grid)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Split the dataset into features (X) and labels (y)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:877\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;66;03m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[0;32m    873\u001b[0m warn_on_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfrequent_if_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    876\u001b[0m }\n\u001b[1;32m--> 877\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_infrequent_categories(X_int, X_mask)\n\u001b[0;32m    885\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:174\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle_unknown \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    170\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound unknown categories \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m in column \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m during transform\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(diff, i)\n\u001b[0;32m    173\u001b[0m     )\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m warn_on_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: Found unknown categories [nan] in column 0 during transform"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from random import choice\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# Create a synthetic dataset\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate date range\n",
    "start_date = datetime(2022, 1, 1)\n",
    "end_date = datetime(2022, 12, 31)\n",
    "date_range = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n",
    "\n",
    "# Generate crime data\n",
    "n_samples = 1000\n",
    "crime_types = ['Theft', 'Assault', 'Vandalism', 'Burglary', 'Drug Offense']\n",
    "locations = ['Dormitory', 'Classroom Building', 'Library', 'Recreation Center', 'Cafeteria']\n",
    "months = [d.strftime('%B') for d in date_range]\n",
    "crime_data = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    date = choice(date_range)\n",
    "    crime_type = choice(crime_types)\n",
    "    location = choice(locations)\n",
    "    month = date.strftime('%B')\n",
    "    latitude = round(np.random.uniform(37.0, 37.1), 6)\n",
    "    longitude = round(np.random.uniform(-122.0, -121.9), 6)\n",
    "\n",
    "    crime_data.append([date, crime_type, location, month, latitude, longitude])\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = ['Date', 'Crime Type', 'Location', 'Month', 'Latitude', 'Longitude']\n",
    "df = pd.DataFrame(crime_data, columns=columns)\n",
    "\n",
    "# Feature engineering: One-hot encode categorical variables\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "encoded_features = encoder.fit_transform(df[['Location', 'Month']])\n",
    "encoded_df = pd.concat([df[['Latitude', 'Longitude']], pd.DataFrame(encoded_features)], axis=1)\n",
    "\n",
    "# Create a base map\n",
    "m = folium.Map(location=[df['Latitude'].mean(), df['Longitude'].mean()], zoom_start=10)\n",
    "\n",
    "# Add markers with intensity based on the number of unique crime incidents\n",
    "locations = list(zip(df['Latitude'], df['Longitude']))\n",
    "heat_map = HeatMap(locations, radius=15)\n",
    "heat_map.add_to(m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('crime_intensity_map.html')\n",
    "\n",
    "# Create a grid of latitude and longitude coordinates\n",
    "latitude_range = (df['Latitude'].min(), df['Latitude'].max())\n",
    "longitude_range = (df['Longitude'].min(), df['Longitude'].max())\n",
    "latitude_step = 0.01\n",
    "longitude_step = 0.01\n",
    "\n",
    "grid_points = []\n",
    "\n",
    "for lat in np.arange(latitude_range[0], latitude_range[1], latitude_step):\n",
    "    for lon in np.arange(longitude_range[0], longitude_range[1], longitude_step):\n",
    "        # Include 'Location' and 'Month' in grid_point_df\n",
    "        grid_points.append((lat, lon, choice(locations), choice(months)))\n",
    "\n",
    "# Create a DataFrame with grid points\n",
    "grid_point_df = pd.DataFrame(grid_points, columns=['Latitude', 'Longitude', 'Location', 'Month'])\n",
    "\n",
    "# Ensure every grid point has a valid month\n",
    "grid_point_df['Month'] = grid_point_df['Month'].apply(lambda x: choice(months) if pd.isna(x) else x)\n",
    "\n",
    "# Make sure the 'Location' and 'Month' columns in grid_point_df have the same categories as in the training data\n",
    "grid_point_df['Location'] = pd.Categorical(grid_point_df['Location'], categories=encoder.categories_[0], ordered=False)\n",
    "grid_point_df['Month'] = pd.Categorical(grid_point_df['Month'], categories=encoder.categories_[1], ordered=False)\n",
    "\n",
    "# Feature engineering for grid points: One-hot encode categorical variables\n",
    "encoded_features_grid = encoder.transform(grid_point_df[['Location', 'Month']])\n",
    "grid_point_encoded_df = pd.concat([grid_point_df[['Latitude', 'Longitude']], pd.DataFrame(encoded_features_grid)], axis=1)\n",
    "\n",
    "# Split the dataset into features (X) and labels (y)\n",
    "X = encoded_df.drop('Crime Type', axis=1)\n",
    "y = df['Crime Type']\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf.fit(X, y)\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(clf_rf, 'random_forest_model.pkl')\n",
    "\n",
    "# Load the trained model from the file\n",
    "clf_rf_loaded = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Use the loaded model to predict the crime types for all grid points\n",
    "predicted_crime_types = clf_rf_loaded.predict(grid_point_encoded_df)\n",
    "\n",
    "# Store the predicted incidents for each grid point\n",
    "predicted_incidents = {}\n",
    "\n",
    "for i, point in enumerate(grid_points):\n",
    "    predicted_incidents[point] = predicted_crime_types[i]\n",
    "\n",
    "# Create a heatmap for predicted crime incidents\n",
    "heat_map_predicted = HeatMap(\n",
    "    data=list(predicted_incidents.keys()),\n",
    "    radius=15,\n",
    "    min_opacity=0.2,\n",
    "    max_val=max(predicted_incidents.values()),\n",
    "    gradient={0.2: 'blue', 0.4: 'lime', 0.6: 'orange', 1: 'red'}\n",
    ")\n",
    "\n",
    "heat_map_predicted.add_to(m)\n",
    "\n",
    "# Save the map with predicted crime incidents to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be12f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
